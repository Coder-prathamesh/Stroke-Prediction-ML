import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df= pd.read_csv('healthcare-dataset-stroke-data.csv')
df.info()
import cufflinks as cf
cf.go_offline()
df['age'].iplot(kind='hist',bins=100,color='green')
df.describe()
sns.heatmap(data= df.isnull(),yticklabels=False,cbar= False,cmap='viridis') #checking for missing values
sns.countplot(x='Residence_type', data= df, palette='coolwarm')
sns.countplot(x='stroke', data= df) # This shows dataset is very imbalance
df.fillna(value= df['bmi'].mean(),inplace=True)
df.drop('id',axis= 1 ,inplace = True) #Since id column is not significant for machine learning
df.head()

#Dummifying the categorical variables
sex = pd.get_dummies(df['gender'],drop_first=True)
married = pd.get_dummies(df['ever_married'],drop_first=True)
job = pd.get_dummies(df['work_type'],drop_first=True)
area = pd.get_dummies(df['Residence_type'],drop_first=True)

df.drop(['gender','ever_married','work_type','Residence_type'], axis=1, inplace= True)
df1=pd.concat([df,sex,married, job, area], axis=1)
df1.drop(['Other'],axis=1,inplace=True)
df1.head()

smoke=pd.get_dummies(df['smoking_status'],drop_first=True)
df1.drop(['smoking_status'], axis=1, inplace= True)
df1=pd.concat([df1,smoke], axis=1)

#Import ML libraries
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, precision_recall_curve
from sklearn.metrics import f1_score, auc, roc_curve, roc_auc_score, classification_report
from sklearn.model_selection import train_test_split

### Standardization
col = ['age', 'avg_glucose_level', 'bmi']
std=StandardScaler()
df1[col] = std.fit_transform(df[col])
df1.head()

# Initiate X and y
X= df1.drop('stroke',axis=1)
y=df1['stroke']

# Handling imbalance data using SMOTE optimizer
sm = SMOTE()
X_bal, y_bal = sm.fit_sample(X, y)

#Splitting the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X_bal, 
                                                    y_bal, test_size=0.25)

#Logistic Regression
logmodel = LogisticRegression()
logmodel.fit(X_train,y_train)
predictions = logmodel.predict(X_test)
logmodel.fit(X_train,y_train)
predictions = logmodel.predict(X_test)
print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))

print('Precision Score: ', precision_score(y_test,predictions))
print('Recall Score: ', recall_score(y_test,predictions))
print('F1 Score: ', f1_score(y_test,predictions))
print('Accuracy Score: ', accuracy_score(y_test,predictions))
print('ROC AUC: ', roc_auc_score(y_test,predictions))
lr_score = accuracy_score(y_test,predictions)
lr_score

#Plotting ROC- AUC curve:

rand_probs = [0 for i in range(len(y_test))]
lr_probs = logmodel.predict_proba(X_test)

#consider only Positive probabilities
lr_probs= lr_probs[:,1]

lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
plt.title('ROC-curve for Logistic regression')
plt.plot(lr_fpr, lr_tpr, 'magenta', label = 'Area Under Curve= %0.4f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

## Decision Trees
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(X_train,y_train)
pred = dtree.predict(X_test)
print(classification_report(y_test,pred))
print(confusion_matrix(y_test,pred))

print('Precision Score: ', precision_score(y_test,pred))
print('Recall Score: ', recall_score(y_test,pred))
print('F1 Score: ', f1_score(y_test,pred))
print('Accuracy Score: ', accuracy_score(y_test,pred))
print('ROC AUC: ', roc_auc_score(y_test,pred))
dtree_score = accuracy_score(y_test,pred)
dtree_score

# Plotting ROC- AUC curve:
rand_probs = [0 for i in range(len(y_test))]
dtree_probs = dtree.predict_proba(X_test)
# consider only Positive probabilities
dtree_probs= dtree_probs[:,1]

dtree_fpr, dtree_tpr, _ = roc_curve(y_test, dtree_probs)
plt.title('ROC-curve')
plt.plot(dtree_fpr, dtree_tpr, 'g', label = 'AUC using SMOTE= %0.3f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()
